# Topic 1: Terraform State — NOTES

## 1. Why Terraform State Exists (The Real Reason)
Terraform is declarative, meaning you define the desired end state. Cloud APIs are often imperative, meaning you issue commands to make changes.
To effectively bridge this gap and execute a desired change (or plan its execution), Terraform needs to know:
Question Purpose Detailed Explanation

"Which cloud
resources belong
to this Terraform
code?"

Mapping (resource
address ←→ provider
ID)

The state file is the single source of truth linking a logical resource name (e.g., aws_instance.web) to the physical, unique ID in the cloud (e.g., i-03294ab12ec). This mapping is crucial for all operations.

"What was the last
known state of
these resources?"

Detecting drift Terraform compares the current configuration (HCL) with the last known state (State File) and the actual state (Cloud API Refresh) to determine if a change is needed.

"What attributes
changed since last
run?"

Plan calculation It allows Terraform to generate an optimal execution plan (e.g., knowing an attribute is only changing its value vs. forcing resource replacement).

"How to destroy
objects cleanly?"

Dependency graph The state contains the full dependency graph, which is necessary to destroy resources in the correct, non-violating order (e.g., VPC endpoints before the VPC).

Terraform keeps all of this in a local or remote state file, usually terraform.tfstate.
Without a state file: Terraform would need to query the entire cloud for every run to build the necessary mapping and graph—an operation that is impossible at scale and prone to errors.

## 2. What Terraform Stores in State
Terraform stores:
- Resource addresses (aws_instance.web) and Resource IDs (i-03294ab12ec).
- Computed attributes (even sensitive ones!) including those generated by the provider after creation, such as public_ip, arn, or database passwords.
- Dependencies (graph): The implicit and explicit relationships between resources.
- Provider version metadata: Which specific provider version was last used to manage the resource, aiding in managing provider upgrades.
- Output values: The final computed values of all defined outputs.
- Module structure: The hierarchical relationship between modules and resources.
- Data source evaluated values: The resolved values obtained from data sources during the last run.
- Counts and for_each iteration mappings: The exact key-to-instance mapping for resources using meta-arguments, ensuring stability when iterating.

Terraform does not store:
- Provider credentials: These are sourced dynamically at runtime.
- Cached plans: Plan files are stored separately, not in the state file itself.
- Locals: Variables defined in HCL for code abstraction.
- Variables (input values are not stored, but resource attributes derived from them are): The raw input variable values are discarded after the plan/apply.

### Sensitive values STILL go into state
Even if you mark a variable or output as: `sensitive = true`

This flag only affects CLI/UI display (hiding it in console output) and Plan file content. It still ends up in the state file in plaintext JSON format.
Rule: Never put plaintext secrets directly in Terraform-managed resources. Instead, use a secure secret manager (like HashiCorp Vault, AWS Secrets Manager, or Azure Key Vault) and reference those secrets in your Terraform configuration dynamically using data sources.

## 3. State Backends — Architecture & Differences
- AWS (S3 + DynamoDB):
  - S3 stores the encrypted state object.
  - DynamoDB Table handles locks: It stores a unique LockID when a process is running. This is a lightweight, low-cost mechanism.
  - IAM used for access. Supports versioning (via S3 bucket feature).
  - Best for: Small →enterprise teams already heavily invested in AWS.
- Azure (Blob Storage + Blob Leases):
  - Blob storage holds the state file.
  - Blob lease = locking mechanism: A time-limited, exclusive lock placed on the blob file by the executing process.
  - Access via AD / Managed Identity. Supports snapshots.
- GCP (GCS + built-in locking):
  - GCS stores the state.
  - Locking implemented via generation IDs: GCP uses an atomic mechanism on the file object itself to ensure only one writer succeeds.
  - Versioning is supported via GCS object versioning.
- Terraform Cloud/Enterprise:
  - Offers Workspace isolation, remote runs (execution off your machine), Drift detection, and State sharing.
  - The most feature-rich option, abstracting the backend details entirely.
  - Great for regulated or multi-team orgs where centralized control and governance are critical.

## 4. State Locking — Deep Internal Explanation
When terraform plan or apply runs:
1. Request Lock: Terraform requests an exclusive lock from the backend (e.g., puts a lock entry in DynamoDB or acquires a Blob Lease).
2. Lock Check: The backend performs an atomic check:
   - If free→ create lock entry, return success.
   - If locked→ return error with the lock ID, user, and timestamp.
3. Execute: Plan/apply executes only if the lock is secured.
4. Release Lock: The lock is finally released, even if the apply fails (using a defer mechanism in the underlying Go code).

Locking Prevents: Two operations (especially two applys) modifying the state file simultaneously, which would lead to a corrupted state and partial resource creation errors.

Why Locking Errors Happen:
- Parallel CI runs: Two separate pipeline stages trigger at the same time.
- Stale lock: The process that acquired the lock terminated abruptly (e.g., machine crashed, OOM killed) before it could release the lock. The backend is left with an existing lock entry.
- Improper permissions: The executing identity has permission to read the state but not to write/delete the lock entry.

Correct Method to Clear a Lock: Only if you are 100% sure no one is actively running Terraform and the lock is confirmed stale.
```bash
terraform force-unlock <LOCK_ID>
```

- The LOCK_ID is provided in the error message when acquiring the lock fails.

## 5. State Drift (One of the Most Common Real-World Problems)
Drift = The actual cloud resource configuration ≠ Terraform state file.

### Drift Sources (The "Why"):
- Manual console changes: A developer or operator modifies an attribute directly via the AWS Console or Azure Portal.
- External autoscaling or rotation: A separate process (e.g., an IAM role rotator, a security scanner) changes a resource attribute without informing Terraform.
- Provider breaking changes: A provider upgrade changes the default value or logic for a computed attribute, causing Terraform to see a difference.
- Failed partial applies: An apply fails midway, and while some resources are created, the state file only records a partial view, leaving the rest of the configuration out of sync.
- Changing attributes ignored from config: Attributes marked with ignore_changes are a form of intentional drift, where the user allows the cloud state to diverge from the HCL config without triggering a re-apply.

### Drift Prevention:
- AWS SCP/Azure RBAC: Use Service Control Policies (SCPs) or Role-Based Access Control (RBAC) to deny direct console/CLI modifications to production resources managed by a specific service principal (the Terraform user).
- GitOps workflows: Enforce that only the CI/CD pipeline (which uses the dedicated, controlled service identity) is allowed to execute terraform apply.
- Immutable infrastructure patterns: Treat all deployed infrastructure as disposable; when a change is needed, destroy the old and create new, which minimizes in-place configuration changes that can lead to drift.
- Do NOT use ignore_changes excessively: Use it only for known, operationally necessary dynamic fields (like timestamps or autoscaling group tags), not as a general way to avoid fixing drift.

## 6. State Manipulation & Repairs (Advanced Operations)
State repair is common in senior-level Terraform work:
a) `terraform state mv` Used to rename resources after HCL code refactor:
```bash
terraform state mv 'aws_db_instance.old[0]' aws_db_instance.new
```
This avoids resource recreation by telling Terraform: "The resource I used to call old is now named new in my code."

b) `terraform state rm` Removes a resource from state without deleting it from cloud.
- Use cases: Move a resource to a different Terraform project/state file; or simply decouple it from Terraform management.

c) `terraform import` When onboarding existing resources:
```bash
terraform import aws_iam_role.my_role my-role-arn
```
Important: import only imports into state → it does not generate the HCL code. Your HCL code must perfectly match the imported resource, or running terraform plan will immediately detect drift and suggest destructive changes.

d) Manual state editing (Not recommended) Only valid for: Emergency fixes, severe Provider bugs (e.g., a missing attribute that the provider doesn't expose a fix for), or highly broken state mappings.
- Process: Use `terraform state pull` (downloads the JSON), fix the JSON, then use `terraform state push <file.json>` (uploads it). Always be extremely careful

## 7. State Security (Important for Interviews)
State contains sensitive info (e.g., database passwords, ARNs, public IPs, computed secrets) →protect it:
- Encrypt S3 buckets with KMS/GCS Encryption: Ensure the state file is encrypted at rest by the backend service.
- Restrict access to backend: Use Principle of Least Privilege (PoLP). The service principal/user should only have read/write access to its own state file location, not the entire bucket.
- Backend credentials should use roles/managed identity, not static keys: Never hard-code access keys. Use IAM roles, Azure Managed Identity, or GCP Service Accounts for dynamic, short-lived credentials.

Never do:
- Storing state in Git: The file is plaintext and contains secrets; a massive security and stability risk.
- Using local state for teams: Leads to corruption, instability, and lack of auditability.
- Hard-coding backend credentials: Violates security best practices.

---

# Topic 1: Terraform State — Scenario-Based Interview Questions

## Question 1 — State Drift in Production EKS Cluster
Your organization manages an EKS cluster and all its node groups using Terraform.
Developers scale node groups manually through the AWS console during bursts.
You notice the following during CI runs:
- terraform plan always shows ASG changes
- Pods sometimes fail because node groups suddenly revert
- A teammate suggests fixing drift using: `terraform apply -refresh-only`

What would you do to resolve this situation permanently, and is `terraform apply -refresh-only` a safe solution here?

### Solution — No, `terraform apply -refresh-only` is not a permanent or safe solution here.

- Why it's unsafe: refresh-only updates the state file to match the actual AWS configuration (the drifted values). This accepts the manually scaled configuration as the new source of truth in the state file.
- The Downside: Terraform effectively forgets the original configuration defined in your code. It fixes the symptom (the plan difference) but allows the underlying problem (manual scaling) to continue, leading to unpredictable configurations and loss of control over the resource lifecycle.

The permanent fix is to recognize that the node group capacity fields (desired_capacity, min_size, and max_size) are dynamic, operational values that should be managed by a dedicated automation tool, not by the static Terraform configuration.

The recommended solution is to use the lifecycle block to tell Terraform to ignore changes to these specific fields.
1. Implement Cluster Autoscaler (CA): Install and configure the Cluster Autoscaler within your EKS cluster. This is the industry standard tool for dynamically managing node group scaling based on pending pod requests.
2. Ignore the Drifted Attributes in HCL: Modify the aws_autoscaling_group resource in your Terraform configuration to ignore external changes to the scaling attributes.

Alternative (Control Plane Fix): Implement AWS IAM policies or SCPs to block developers from making direct console modifications to the EKS node group ASGs. This enforces the GitOps principle that infrastructure changes must only come through the CI/CD pipeline.

## Question 2 — Provider Upgrade Broke the State
Your team upgrades aws provider from version 3.x to 5.x.
After upgrade, Terraform plan wants to recreate 40% of infrastructure because some resource schemas changed.
How do you diagnose this situation and prevent unwanted recreation? Describe the safest migration approach.

### Solution — The sudden proposal to recreate 40% of infrastructure after a major provider jump (v3.x to v5.x) is almost certainly due to breaking schema changes introduced by the provider developer (HashiCorp/AWS). These changes affect resource defaults, attribute types, or mark fields as "ForceNew," compelling Terraform to destroy and recreate the cloud resource.

1. Diagnosis and Root Cause
Generate a Structured Plan: The first step is to accurately diagnose which resources are affected and why they require recreation.
```bash
terraform plan -out=tfplan
terraform show -json tfplan | jq '.resource_changes[] | select(.change.actions[] == "delete" or .change.actions[] == "replace")'
```
Analyze requiresNew: Examine the JSON output for attributes marked with "requiresNew": true. This explicitly indicates which specific HCL configuration changes necessitate resource replacement.

2. Safest Migration Approach
The goal is to prevent unwanted destruction and manage the schema shift non-destructively, often using the provider's official migration guide as the primary source of truth.

1. Incremental Upgrade and Isolation:
   - Do not upgrade all modules simultaneously. Upgrade the provider version one module or service at a time.
   - Perform a plan after each module upgrade to isolate the impact.
2. Utilize ignore_changes (Temporary):
   - For attributes that are forcing recreation but hold acceptable values in the cloud (e.g., tag schema changes), use `lifecycle { ignore_changes = [...] }` temporarily during the migration. This prevents the immediate recreation, allowing you to gradually update the HCL code to the new schema later.
3. Code Migration:
   - Refer to Provider Guides: Directly apply the necessary syntax and schema changes

---

# Topic 2: Terraform Modules — NOTES

## 1. Why Modules Exist (Real-World View)
Modules are not just about code reuse; they are the primary mechanism for standardization and governance in IaC.
Modules solve:
- DRY (Don't Repeat Yourself) across environments: Ensures that, for instance, every production VPC is created using the same, verified block of code, regardless of the region it's in.
- Standardization of infra patterns: Enforces company standards (e.g., mandatory tags, specific security group rules, required logging settings) across all deployments.
- Abstraction of complexity: Hides the verbose, provider-specific resource definitions behind a simple, high-level interface (e.g., using a database module instead of a 200-line aws_rds_instance block).
- Reusable building blocks for teams: Enables a platform team to build secure, golden-path infrastructure modules for application teams to consume.
- Versioning & lifecycle management: Changes to a core piece of infrastructure (like networking) can be tested as a new module version (v2.0.0) before being rolled out to downstream applications.
- Separation of concerns: Keeps networking logic separate from compute logic, separate from data logic.

Modules enforce discipline by making infra predictable and reducing drift by minimizing manual copy-pasting of potentially inconsistent code blocks.

## 2. Types of Modules
1. Root Modules:
   - The entry point for a Terraform execution (where terraform init and apply are run).
   - Always contains the backend configuration.
   - Contains environment-specific overrides (e.g., variable values specific to prod or dev).

2. Reusable/Child Modules:
   - Generic, parameterized modules consumed by root modules via the module block.
   - They define the infrastructure pattern.

3. External / Registry Modules:
   - Versioned modules sourced from a public repository (Terraform Registry) or a private repository (Git, S3/GCS).
   - Must be locked with a semantic version constraint: `version = "~> 5.0"`. Using the tilde-greater-than operator (~>) is critical; it ensures patch and minor updates are allowed, but breaking major changes are prevented.

## 3. Good Module Design Principles (What senior engineers are judged on)
- Keep modules small and focused: Modules should follow the Single Responsibility Principle (SRP).
  - Bad: one module provisioning VPC + RDS + EKS.
  - Good: separate modules with clean interfaces (a vpc module outputs IDs, which are inputs for the eks module).
- Inputs should be minimal but expressive: Use sane defaults within the module for non-critical attributes.
  - Too many variables = overwhelming usage, higher chance of misconfiguration.
  - Too few variables = inflexible module, forcing users to fork it.
- No environment-specific logic inside modules: This is a critical separation of concerns.
  - Avoid: `if var.env == "prod" ....` The reusable module should not care where it is deployed.
  - Environment logic belongs in the root module via HCL logic (count, for_each, local variables, or input files).
- Outputs should expose only what is necessary: Expose only the minimum required values for dependent services.
  - Example: Output the vpc_id and public_subnet_ids.
  - Avoid exposing hundreds of attributes (tight coupling), which makes refactoring the module harder later.
- Use for_each inside modules to scale: Use meta-arguments for scaling resources based on a map input.
  - Example: A single EKS module can provision multiple node groups using a node_groups map input.
- Use locals for transformation logic: Centralize complex string operations, name generation, and input value calculations in locals.tf. This improves readability and maintains a clean separation from input variables.
- Use module versioning: Pinning versions ensures that downstream consumers are not accidentally broken by module maintainers releasing changes.

## 4. What NOT to put in modules (Common anti-patterns)
These items tie the generic module to a specific execution context:
- Backend configuration: Must only live in the root module.
- Provider blocks: Child modules inherit the provider configuration from the root module. Defining them inside can lead to unexpected provider version mismatches.
- Environment-specific values: (See Principle 3).
- Secrets: Secrets should be sourced dynamically (e.g., using data blocks for Vault/SSM/Secrets Manager) in the root module, not hardcoded or managed within the child module itself.
- Hardcoded ARNs or region names: Breaks the reusability. Use variables or data sources (like aws_region) instead.
- Large monolithic modules: Violates SRP (see Principle 3).

## 5. Module Lifecycle & Versioning Strategy
A robust module versioning strategy is essential for organizational stability:
1. Semantic Versioning: Modules should strictly follow SemVer (MAJOR.MINOR.PATCH).
   - PATCH (0.0.1): Non-breaking bug fixes.
   - MINOR (0.1.0): New features, backward-compatible.
   - MAJOR (1.0.0): Breaking changes (e.g., removing an input variable, changing an output structure, or replacing a resource).

2. Release Flow:
   - A new MAJOR version (v2.0.0) must first be tested and deployed in Development, then Staging, and finally Production.
   - Always pin module versions in environment root modules to prevent accidental, untested upgrades.

## 6. Testing Modules (Senior Level Expectation)
Module testing ensures reliability and confidence in deployments.
- Integration Testing (Go-based):
  - Use Terratest (Go-based framework) to deploy the module in a real, isolated cloud account, run validation checks (e.g., check that the ALB is publicly accessible), and then tear it down. This is the gold standard for testing infrastructure logic.
- Linting and Validation:
  - Use pre-commit hooks to enforce `terraform fmt`, `terraform validate`, and run security checks like tfsec or checkov before code is committed.
- CI Pipeline:
  - The pipeline should run `terraform init -backend=false` (to avoid state initialization) and a full terraform plan on a minimal, clean example configuration of the module.

---

# Topic 3: WORKSPACES VS DIRECTORY STRUCTURE

## 1. Terraform Workspaces — What They Actually Do
Workspaces are a feature that enables multiple, distinct state files to be managed by a single Terraform configuration folder.
- `default` workspace → default state file.
- `terraform workspace new dev` → creates a new state file named after the workspace.

Behind the scenes, the remote backend stores these states using unique keys/prefixes, often in a structure like:
```
<backend_prefix>/env:/dev/terraform.tfstate
<backend_prefix>/env:/prod/terraform.tfstate
```
Crucially, all workspaces share the exact same HCL code in the working directory.

## 2. What Workspaces Are NOT Suitable For (Critical)
Workspaces should never be used for separation between long-lived, independent production environments (dev/stage/prod).
The inherent problem is the Shared Blast Radius:
- Shared Code: All environments use the same HCL code. A buggy change to that code affects all workspaces simultaneously.
- Shared Pipeline/Permissions: Often, one CI/CD pipeline runs all workspaces. An error in scripting or variable mapping can lead to a cross-environment mistake (e.g., applying the dev change to the prod workspace).
- Shared Backend/State Isolation: While the state files are separate, they reside in the same backend resource (e.g., one S3 bucket). This makes access control, audit trails, and deletion policies harder to manage granularly.

This risk profile is unacceptable for regulated or large-scale production environments.

## 3. Why Directory Structure Wins in Real Companies
The Directory Structure pattern is the industry standard for production infrastructure because it enforces complete physical separation and isolation of blast radius.

Example structure:
```
live/
  dev/   // A completely separate Terraform project
    main.tf
    backend.tf // Separate backend config
    variables.tf
  prod/  // Another completely separate Terraform project
    main.tf
    backend.tf // Separate backend config
    variables.tf
```

Key Benefits of Directory Structure:
- Independent State & Backend: Each directory uses its own, unique S3 bucket and DynamoDB lock table (or unique keys).
- Independent Permissions: IAM/RBAC can be strictly applied per directory/state (e.g., the CI role for dev cannot access the prod S3 bucket).
- Independent Lifecycle: You can upgrade providers, change module versions, or fix bugs in dev without affecting the prod directory's state or execution.
- Clear GitOps Boundary: Changes to live/prod/ require a specific review and approval gate, separate from changes to live/dev/.

## 4. When to Use Workspaces (Legitimate Use Cases)
Workspaces are ideal for situations where you need to deploy the exact same infrastructure pattern multiple times within a single organizational boundary (i.e., short-lived or multi-tenant deployments).

Use workspaces when:
- Ephemeral Environments (PR/Feature): A developer opens a Pull Request (PR). A unique, short-lived infrastructure preview is spun up, and its workspace is named pr-123. The infra is identical to dev, but isolated.
- Multi-Tenancy/Customer Isolation: A SaaS platform provisions the same stack (VPC, DB, Lambda) for many customers. Each customer becomes a unique workspace (e.g., customer-a, customer-b). This simplifies management because the code is identical, only the input variables change.
- Prototyping/Sandbox: Quick testing in a sandbox account where state separation is needed but isolation isn't mission-critical.

---

# Topic 2&3: Scenario-Based Interview Questions

## Question 1 — Multi-Region Deployment Strategy
You need to deploy the same VPC module to 3 regions and 3 environments.
A junior engineer suggests: "Let’s just create 9 Terraform workspaces."
What is the correct approach, and why not use workspaces?

### Solution — Workspaces are unsuitable for this task because they create a Shared Blast Radius and lack configuration isolation.
- Shared Code: All 9 deployments would run off the same HCL code. An error in that code could simultaneously compromise all 9 environments/regions.
- Unique Configuration: Regions require unique, non-negotiable inputs (e.g., non-overlapping CIDR blocks and region-specific AZ names). Workspaces cannot effectively manage this degree of fundamental configuration difference without complex, error-prone logic.

Correct Approach: Directory Structure + Modules
The solution is to use a Directory Structure to enforce separation and a Module for reuse.
1. Isolation (Directory Structure): Create separate, independent root Terraform directories for each environment/region combination (e.g., `prod/us-east-1/`, `dev/eu-west-1/`). Each directory gets its own independent state file, backend, and permissions boundary.
2. Reuse (VPC Module): Create a single, generic VPC Module.
3. Configuration (Variables): Each leaf directory simply calls the VPC module and passes in the unique, non-shareable variables (CIDR, region name, etc.).

This approach maximizes safety and decouples the blast radius.

## Question 2 — Versioning & Module Stability Issue
A team updated a shared VPC module from v1.2.0 to v2.0.0.
This introduced a breaking change that caused internet gateways and NAT gateways to be recreated, breaking production traffic.
How do you prevent this kind of issue in the future?

### Solution — The outage was caused by a violation of Semantic Versioning (SemVer) and a lack of module validation.
1. Enforce Semantic Versioning
   - Module maintainers must adhere to SemVer discipline:
     - MAJOR Version (e.g., v1 → v2): Reserved only for breaking changes (like forcing resource replacement).
     - MINOR/PATCH (e.g., v1.2 →v1.3): Must be backward-compatible and safe.
2. Pin Versions in Consumption
   - Consumer code must use the tilde-greater-than operator (`~>`) to prevent automatic upgrades to MAJOR versions, locking the environment to stable updates only:
```hcl
Terraform
module "vpc" {
  # ...
  version = "~> 1.2.0" # Allows 1.2.x, but strictly blocks the breaking 2.0.0.
}
```
3. Implement Integration Testing
   - Use Terratest or similar frameworks to deploy and test the module in an isolated account.
   - Tests must specifically run terraform plan between versions to assert that no critical resource (like NAT/IGW) is scheduled for destruction ("delete" or "replace").
4. Staged Rollout
   - Require the module upgrade to be successfully reviewed and applied in Dev and Stage environments before deployment to Production.

## Question 3 — Module Anti-Pattern Detection
A module contains: Backend configuration, Provider blocks, Hardcoded region, Environment-based branching (`if var.env == "prod"`), Secrets passed as variables

Identify what is wrong and how to refactor it.

### Solution — Problems (Violations)
1. Backend & Provider Blocks: Breaks reusability; these belong in the Root Module.
2. Hardcoded Region/Environment Branching: Breaks portability; module becomes inflexible and non-generic.
3. Secrets Passed as Variables: Critical security flaw; secrets will be written in plaintext to the state file.

### Refactor Strategy
1. Execution: Move backend and provider blocks to the root module.
2. Configuration: Remove environment branching. Use input variables (e.g., `var.region`) passed from the root module for all differences.
3. Security: Integrate secrets directly in the root module using data sources (e.g., AWS Secrets Manager, Vault) and pass only the resolved, non-state-stored value to the module.

---

# Topic 4: Managing Terraform for Multi-Account Envs — NOTES

## 1. Why Multi-Account Management Matters
Multi-account architecture is the security baseline for any organization of significant size. It shifts the burden of security from complex network rules to fundamental boundary isolation enforced at the organizational level (AWS Organizations/Azure Management Groups).
- Security boundaries: Production is isolated from Dev, QA, Sandbox, Shared Services, Logging, and Security tooling. This ensures that a breach or misconfiguration in a lower environment cannot be escalated to the production account, maximizing the containment of security incidents.
- Blast radius reduction: A bad IAM policy, accidental delete, or failed Terraform apply in Dev must NEVER affect Prod. The failure domain is limited to a single account, preventing global outages.
- Billing & compliance: Enables precise cost allocation and governance. Each account can have mandatory security controls enforced via Service Control Policies (SCPs) (AWS) or Azure Policy, limiting resource deployment to approved types and regions.
- Least privilege: Teams only get access to the accounts they need, often enforced via central identity management (AWS SSO/IAM Identity Center) and Permission Boundaries attached to deployed roles.

## 2. Why Multi-Account Management Matters
1. Each account gets its own state files (Maximum Isolation): Never use a single state to manage resources in multiple accounts. State files are the single source of truth; mixing them creates a single point of failure and makes blast radius definition impossible.
2. Use separate backends per account (Independent Failure Domain): The state file, its lock, and its encryption key must be unique per target account.
   - Example (AWS): Each account should point to a unique S3 bucket and DynamoDB lock table. This ensures a lock failure in Dev does not hold up a Prod deployment, and an accidental S3 deletion in Dev cannot affect the Prod state.
3. Never mix accounts inside the same execution: A single terraform apply should target only one AWS account via its primary provider configuration.
   - If cross-account data is needed (e.g., to reference a central VPC ID), use a data source via role assumption/provider alias, not remote state access.
4. Use modules for reuse, not to mix accounts (Maintain Portability): Modules must remain generic and account-agnostic. All environment and account-specific details (Account ID, IAM role ARN, Region, CIDR) are supplied as variables by the root module directory.

## 3. Authentication Patterns in Multi-Account Terraform
Pattern 1 — Role Switching (Most Common & Secure): The CI/CD system or user authenticates once to a root/identity account profile, then uses AWS STS to assume a role in the target account for the duration of the run.
```hcl
provider "aws" {
  # ... base credentials/profile
  assume_role {
    role_arn = var.target_role_arn # ARN of the role in the TARGET account
  }
}
```
- Benefit: Credentials are short-lived, centrally controlled, and the blast radius is strictly defined by the role_arn policy.

Pattern 2 — AWS SSO / IAM Identity Center: Standard for local developer machines. Users login once via `aws sso login`, which manages temporary tokens, referenced by a simple profile name.

Pattern 3 — Dedicated CI/CD Roles per Environment (OIDC): CI platforms (GitHub Actions, GitLab) use OpenID Connect (OIDC) to directly assume a role within the Dev, Staging, or Prod account. This avoids storing any static keys and removes the dependency hop through a separate root/management account for the pipeline itself.

## 4. Directory Structure for Multi-Account Terraform
The Directory Structure is the mandatory pattern for multi-account/multi-environment management as it provides the necessary isolation boundary.

Example:
```
live/
  dev/
    vpc/ # Project 1: Dev Account, VPC Stack
    eks/ # Project 2: Dev Account, EKS Stack
  prod/
    vpc/ # Project 3: Prod Account, VPC Stack
    eks/ # Project 4: Prod Account, EKS Stack
modules/
  vpc/ # Reusable VPC logic (generic HCL)
```

- Isolation: Each leaf folder (live/prod/vpc/) is a separate root project with its own:
  - `backend.tf` configuration pointing to a unique state location.
  - `provider.tf` defining the unique assume_role ARN for that account.
  - Independent pipeline and access control.

## 5. Terragrunt (Best Tool for Multi-Account Mgmt)
Introduction to Terragrunt:
Terragrunt is a thin, open-source wrapper around Terraform that provides the orchestration layer necessary for enterprise-scale deployments. Its primary function is to enforce the DRY (Don't Repeat Yourself) principle by eliminating the repetition of backend and provider configuration blocks across dozens of environment/account folders.

It solves the pain points of the Directory Structure by:
1. Configuration Inheritance: Defining repetitive blocks once in a parent .hcl file and inheriting them down the folder hierarchy.
2. Automated Backend Generation: Dynamically generating unique S3 state keys, preventing copy-paste errors.
3. Cross-Module Dependencies: Orchestrating the order of deployment across separate Terraform state files.

Terragrunt Example: DRY Backend
In a standard Terraform directory structure (Refer to the dir structure, shared above), you'd repeat this backend block in every single project:
```hcl
# Repeated in live/dev/vpc/main.tf, live/prod/vpc/main.tf, etc.
terraform {
  backend "s3" {
    bucket = "company-tfstate-bucket"
    key = "dev/vpc/terraform.tfstate" # Different key for each folder
    region = "us-east-1"
    # ... DynamoDB and role_arn are also repeated
  }
}
```
With Terragrunt, you define the common block once in a parent `terragrunt.hcl`:
```hcl
# live/terragrunt.hcl (Parent Configuration)
remote_state {
  backend = "s3"
  config = {
    bucket = "company-tfstate-bucket"
    region = "us-east-1"
    # The key is automatically generated based on the folder path
    key = "${path_relative_to_include()}/terraform.tfstate"
  }
}
```
The leaf modules (e.g., `live/prod/vpc/terragrunt.hcl`) then only contain the code for the module call, inheriting the backend automatically:
```hcl
# live/prod/vpc/terragrunt.hcl (Leaf Configuration)
include {
  path = find_in_parent_folders() # Inherit configuration from parent HCL files
}
terraform {
  source = "../../modules/vpc" # Point to the generic module HCL
}
inputs = {
  vpc_cidr = "10.0.0.0/16"
  environment = "prod"
}
```
When `terragrunt apply` runs in the vpc folder, it automatically generates the required backend block, using the path `prod/vpc/terraform.tfstate` as the unique key.

## 6. State, Security & Governance
- Separate state buckets + KMS keys per account: The KMS key used for encrypting the S3 state file must also be unique per environment to prevent a key compromise from exposing multiple environments' state.
- Apply SCPs in the Org Management account: Service Control Policies (SCPs) are mandatory to enforce guardrails (e.g., blocking the deletion of CloudTrail or mandatory S3 encryption) across all child accounts before Terraform runs.
- Centralized logging & monitoring account: Terraform in workload accounts must be configured to provision logs (e.g., CloudTrail, VPC Flow Logs) to a centralized, secured logging account that no individual team can access or tamper with.

## 7. CI/CD Strategy for Multi-Account Terraform
- One pipeline per account (or environment): This ensures that production changes have stricter approval gates, use a restricted role, and are physically separated from development deploys.
- Use OIDC instead of static credentials: OIDC is the most secure method for CI/CD, as it allows the pipeline to dynamically assume the target account role without storing long-lived AWS keys in the CI secrets manager.

---

# Topic 4: Scenario-Based Interview Questions

## Question 1 — Wrong Account Deployment Accident
Your organization uses multiple AWS accounts: Dev, QA, Prod.
A new engineer accidentally deployed resources meant for Dev into the Prod account because their local AWS CLI profile was misconfigured.
What changes would you recommend to prevent this permanently?

### Solution — The failure was due to relying on local, mutable authentication (AWS CLI profile).
1. Enforce CI-Only Applies: Use IAM policies/SCPs to deny local developer roles the permission to run apply in the Production account. All Prod deploys must be serialized via CI.
2. Isolate Authentication:
   - Provider Lock: Hardcode the Production root module's provider block to assume a unique Prod CI Role ARN. This decouples execution from the user's local profile.
   - OIDC: Use OpenID Connect (OIDC) for the CI pipeline to securely assume this restricted Prod Role.
3. Validate Target Account: Add a pre-apply check using the data `aws_caller_identity` block in the root module. Fail the pipeline immediately if the returned Account ID does not match the expected Production ID.

This enforces a controlled, verifiable, and isolated execution path for production infrastructure.

## Question 2 — Multi-Account Drift Detection
Security team wants weekly drift reports for every AWS account managed by Terraform.
How would you implement this?

### Solution — Implement a scheduled CI job that iterates over each environment directory, using OIDC role assumption to securely execute a non-destructive plan in the target account.
The core command is `terraform plan -detailed-exitcode`. The pipeline checks for Exit Code 2 (Drift Detected); upon detection, it triggers an immediate notification and saves the `tfplan.out` as a secure artifact for mandatory manual review.

## Question 3 — Terragrunt vs Pure Terraform for Multi-Account Setup
Your infra spans 12 AWS accounts and 80+ Terraform stacks.
Developers complain about duplicated backend configs and providers everywhere.
Should you move to Terragrunt? Why or why not?

### Solution — Terragrunt acts as a necessary orchestration layer that enforces the DRY (Don't Repeat Yourself) principle, dramatically reducing the maintenance burden in large organizations.
1. Eliminates Duplication (DRY):
   - Backend and Provider Blocks: Terragrunt allows you to define the standard S3 backend configuration (bucket name, DynamoDB table, region) and the assume_role configuration for the AWS provider once in a parent .hcl file. Child stacks (the 80+ stacks) simply inherit this configuration. This directly solves the developers' complaint.
2. Manages Dependencies:
   - Cross-State Orchestration: Terragrunt uses the dependency block to manage deployment order (e.g., ensuring a VPC stack is deployed successfully before the dependent EKS stack begins). This is complex and error-prone to manage manually with pure Terraform across many accounts.
3. Hierarchical Configuration:
   - It enables clean configuration layering. You can define global defaults at the top level, inject account-specific details in the middle (live/prod/terragrunt.hcl), and define module inputs at the bottom (live/prod/vpc/terragrunt.hcl).
4. Cleaner Execution:
   - Commands like `terragrunt run-all apply` allow you to deploy the entire infrastructure across multiple accounts in the correct, calculated order, making CI/CD easier to manage than scripting dozens of individual `terraform apply` commands.

### Why Pure Terraform Fails
At this scale (80+ stacks), maintaining pure Terraform leads to:
- Unmanageable Repetition: You would have 80+ directories, each containing a duplicated backend block. Changing the S3 bucket name or the locking table name would require modifying dozens of files, introducing high risk of error.
- Lack of Orchestration: There is no built-in way to orchestrate the deployment order between independent Terraform stacks, forcing reliance on brittle shell scripts in the CI/CD pipeline.

---

# Topic 5: Security & Secrets in Terraform — NOTES
Terraform's role is to securely reference secrets, not to store, rotate, or generate long-term sensitive data. Misuse of variables or state is the primary cause of secret exposure.

## 1. Core Rule: Never Store Secrets Directly in Terraform
This rule is about preventing the persistence of secrets within the IaC code or its artifacts.

Action | Why (Technical Risk)
--- | ---
❌ Hardcode in .tfvars or Modules | Commits plaintext secrets to version control (Git), violating security baseline.
❌ Pass via CLI (-var="password=...") | Leaks secrets into shell history and CI/CD logs (even if masked).
❌ Output secrets (output "password" { ... }) | Writes secrets to the terminal and remote state; exposes secrets to anyone who can read the output or state.
❌ Store in Remote State/Plan Files | Sensitive data persists on S3/GCS/Azure Blob, requiring high-level security controls for all state files.

DO: Fetch secrets at runtime from secret stores via data sources. This keeps Terraform stateless with respect to sensitive values.

## 2. Why "sensitive = true" Does NOT Protect Secrets
Marking an input variable or output as `sensitive = true` provides minimal protection and is often misunderstood.
```hcl
variable "db_password" {
  type = string
  sensitive = true
}
```
- Effect: It only hides the value in the CLI/UI output (Plan/Apply logs) and masks it within binary Plan files (.tfplan).
- Technical Risk: The secret STILL goes into:
  1. `terraform.tfstate`: Stored in plaintext JSON, immediately compromising the state file.
  2. Backend Storage: Persists in the S3 bucket, DynamoDB, or Terraform Cloud workspace.

Conclusion: Terraform is NEVER the source of truth for secrets. `sensitive = true` is a UI tool, not a security control.

## 3. Managing Secrets the Correct Way (Cloud-Native)
The recommended pattern is Secure Retrieval at Runtime.
Use cloud-native secret stores (The Source of Truth):
- AWS: Secrets Manager (recommended for rotation) or SSM Parameter Store (simpler, non-rotating secrets).
- Azure: Key Vault.
- GCP: Secret Manager.

Fetch secrets via data sources (The IaC Integration):
```hcl
data "aws_secretsmanager_secret_version" "db" {
  # Terraform executes an API call at runtime to fetch the secret version
  secret_id = "prod/db/password"
}
resource "aws_db_instance" "prod_db" {
  # Reference the retrieved value. It will be written to state by the provider.
  password = data.aws_secretsmanager_secret_version.db.secret_string
  # ...
}
```
Rule: While the provider will write the computed value into the state, the secret's lifecycle and rotation are managed entirely by the external store.

## 4. Vault + Terraform Integration (Enterprise Pattern)
Vault is ideal for managing dynamic secrets and centralized security policy across hybrid clouds.
- Terraform's Role (Provisioning):
  - Provision Vault policies, auth methods, and secret engines (e.g., configuring the AWS IAM role engine).
  - Manage roles within those engines (e.g., defining a DB role that generates temporary credentials).
- Vault's Role (Operation):
  - Vault handles secret generation and rotation.
  - Applications (not Terraform) request secrets from Vault at runtime, receiving short-lived, ephemeral credentials.

Separation of Concerns: Terraform provisions the infrastructure for secrets management, and Vault handles the operational secret flow. Terraform stays out of the sensitive runtime path.

## 5. Secure Pipelines for Terraform
CI/CD pipelines are the primary execution point and must enforce zero-trust principles.
- Authentication: Must use short-lived credentials (OIDC → AssumeRole) that are unique per environment, never static keys.
- Logs & Artifacts: Use platform features to mask sensitive logs (even if marked sensitive in HCL). Never store plan files as CI artifacts, as they contain the secrets that are supposed to be hidden.
- Execution Control: CI-only applies for production environments, enforced by IAM/RBAC that denies local user apply permissions.

## 6. Backend Security Patterns
The remote backend (where state resides) is the most critical asset to secure.
- AWS S3 Backend:
  - Encryption: Mandatory SSE-KMS (Server-Side Encryption with KMS).
  - Resilience: Enable S3 versioning (for rollbacks) and configure object locks (for compliance).
  - Access Control: Use least-privilege IAM policies specifically scoped to the state path and DynamoDB lock table. Block public access.
- Azure/GCP:
  - Use Managed Identities (Azure) or Service Accounts (GCP) for authentication instead of static keys.
  - Enable native storage encryption and compliance locks.

## 7. Encryption & KMS Best Practices
Encryption must be layered and isolated.
- Encrypt Everything: Terraform state, remote storage, and the secret store entries themselves (Secrets Manager, Key Vault).
- KMS Isolation: Never use AWS-managed default KMS keys for production environments. Use Customer Managed Keys (CMKs) that are unique per environment or per application (e.g., one CMK for prod-db-state, another for prod-vpc-state). This allows fine-grained IAM control over key usage.

## 8. Multi-Cloud Secret Workflow Patterns
The strategy depends on the organizational footprint.

Pattern | Description | Terraform's Role
--- | --- | ---
A: Cloud-Native Per-Cloud | AWS → Secrets Manager; Azure → Key Vault. Apps fetch directly from their native cloud provider. | Provisions the required Secret Manager resource and fetches references via data sources.
B: Vault as Global Secrets Platform | Vault is the centralized store. Clouds may hold short-lived, temporary credentials only. | Provisions the Vault platform/engines but does NOT fetch operational secrets.

In any model, the goal is for Terraform to fetch no operational secrets.

## 9. Key Enterprise-Level Anti-Patterns (Must Avoid)
- Passing secrets through modules: A massive risk; secrets can be inadvertently exposed in outputs or logs deep within the module stack.
- Outputting sensitive keys: Immediately writes the secret to the state file and CLI, defeating all security measures.
- Storing secrets encrypted with KMS but committing ciphertext to Git: The KMS key ARN is typically stable. If the key is compromised, the ciphertext (now in Git) is immediately compromised. The decryption process should only run in a secured CI environment.
- Using Terraform for secret rotation: Terraform is transactional (apply/destroy). Rotation should be handled by dedicated services (Secrets Manager, Vault) on a scheduled basis.

---

# Topic 5: Scenario-Based Interview Questions

## Question 1 — Secrets Appearing in Terraform State
A developer reports that the RDS password is visible in the Terraform state file even though the variable is marked as:
```hcl
variable "db_password" {
  type = string
  sensitive = true
}
```
Why is this happening, and what is the correct solution?

### Solution
- `sensitive = true` only hides values in the CLI, not in state.
- Terraform always stores computed values in state — including passwords.
- Correct solution:
  - Store secrets in Secrets Manager / SSM / Vault
  - Fetch them via data sources
  - Never embed secrets directly in Terraform-managed resources.

## Question 2 — Dev Secrets Accidentally Used in Prod
A module takes a `db_password` variable.
A developer mistakenly supplies a Dev password for the Prod environment.
How do you redesign your secret workflow to prevent this permanently?

### Solution — The issue is that the secret was a portable variable, allowing the wrong value to be injected.
The permanent fix is to enforce IAM-based isolation, making the deployment fail if the wrong secret source is referenced.
1. Eliminate the Variable: Stop passing secrets as input variables.
2. Isolate Stores: Prod secrets must reside only in the Prod Secret Manager.
3. Use Data Sources: The Prod deployment's Terraform configuration fetches the secret using a data source that is inherently authenticated to the current environment.
4. Security Gate: Since the Prod CI Role has no permission to read the Dev Secret Manager, an attempt to access the Dev secret will result in an IAM denial, permanently preventing the mistake.

This forces the deployment to rely on cloud security boundaries, not human memory.

## Question 3 — Vault Dynamic Credentials Misuse
A team wants Terraform to fetch dynamic database credentials from Vault and pass them into ECS tasks.
Why is this wrong, and what is the correct approach?

### Solution — The team's workflow is fundamentally wrong because Terraform is not an operational tool for managing ephemeral secrets.

Why This is Wrong
1. State Leak: Fetching the dynamic secret forces Terraform to write that short-lived password into the persistent, static `terraform.tfstate` file in plaintext. This is a severe, permanent secret leak.
2. Rotation Failure: Terraform only runs once. It cannot update the running ECS task when Vault automatically rotates the credential moments later, causing the application to fail.

Correct Approach: Application-Side Fetching
The solution is to shift the responsibility for fetching the ephemeral secret entirely to the runtime environment.
1. Terraform's Role: Terraform only provisions the access infrastructure (i.e., creates the Vault Roles and Policies).
2. Application's Role: The ECS Task (or an integrated Vault Agent) uses its IAM credentials to fetch the dynamic credential directly from Vault at runtime via the API.

Conclusion: Terraform creates the mechanism; the application manages the secret, ensuring it remains short-lived and out of the state file.

## Question 4 — Backend Security Review
Security team asks how you protect Terraform remote state stored in S3.

### Solution — Protecting state requires layered controls on the S3 bucket.
1. Encryption: Enable SSE-KMS using per-environment Customer Managed Keys (CMKs). This isolates decryption capability.
2. Access: Implement least-privilege IAM policies and S3 Public Access Block. Only the dedicated CI/CD role should have access.
3. Integrity: Use a DynamoDB table for state locking and enable S3 Versioning for recovery from corruption or deletion.
4. Auditing: Use CloudTrail to log all access and API activity on the S3 bucket and DynamoDB lock table.
